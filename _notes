# The Top 8 Clustering Algorithms
https://www.freecodecamp.org/news/8-clustering-algorithms-in-machine-learning-that-all-data-scientists-should-know/

K-means clustering algorithm
- centroid-based algorithm and the 
- simplest unsupervised
- best used on smaller data sets because it iterates over all of the data points. 
- doesn't scale well. ⛔️

DBSCAN clustering algorithm
- DBSCAN stands for density-based spatial clustering of applications with noise. It's a density-based clustering algorithm, unlike k-means.
- goodfor finding outliners ✅
- This algorithm is better than k-means when it comes to working with oddly shaped data.
- two parameters to:
  - minPts (the minimum number of data points)
  - eps (the distance in the same area).

Gaussian Mixture Model algorithm
- problems with k-means -> circular format
- This models fix. ✅
- uses multiple Gaussian distributions.
- There are several single Gaussian models
- model calculates the probability that a data point belongs to a specific Gaussian distribution and that's the cluster it will fall under.

BIRCH algorithm
- works better on large data sets than the k-means algorithm. ✅
- It breaks the data into little summaries that are clustered instead of the original data points.
- The summaries hold as much distribution information about the data points as possible.
- commonly used with other clustering algorithm because the other clustering techniques can be used on the summaries generated by BIRCH.
- The main downside of the BIRCH algorithm is that it only works on numeric data values. You can't use this for categorical values unless you do some data transformations. ⛔️

Affinity Propagation clustering algorithm
- completely different from the others
- Each data point communicates with all of the other data points to let each other know how similar they are and that starts to reveal the clusters in the data. You don't have to tell this algorithm how many clusters to expect in the initialization parameters. ✅
- As messages are sent between data points, sets of data called exemplars are found and they represent the clusters.
- An exemplar is found after the data points have passed messages to each other and form a consensus on what data point best represents a cluster.
- When you aren't sure how many clusters to expect, like in a computer vision problem, this is a great algorithm to start with.

Mean-Shift clustering algorithm
- This is another algorithm that is particularly useful for handling images and computer vision processing.
- Mean-shift is similar to the BIRCH algorithm because it also finds clusters without an initial number of clusters being set.
- This is a hierarchical clustering algorithm, but the downside is that it doesn't scale well when working with large data sets. ⛔️
- It works by iterating over all of the data points and shifts them towards the mode. The mode in this context is the high density area of data points in a region.
- That's why you might hear this algorithm referred to as the mode-seeking algorithm. It will go through this iterative process with each data point and move them closer to where other data points are until all data points have been assigned to a cluster.

OPTICS algorithm
- density-based
- similar to DBSCAN, but better because, can find meaningful clusters in data that varies in density. ✅
- but slower ⛔️
- This makes it easier to detect different density clusters. 
- The OPTICS algorithm only processes each data point once, similar to DBSCAN (although it runs slower than DBSCAN). 
- There's also a special distance stored for each data point that indicates a point belongs to a specific cluster.

Agglomerative Hierarchy clustering algorithm
- This is the most common type of hierarchical clustering algorithm.
- It's used to group objects in clusters based on how similar they are to each other.
- This is a form of bottom-up clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.
- At each iteration, similar clusters are merged until all of the data points are part of one big root cluster.
- Agglomerative clustering is best at finding small clusters. ✅⛔️
- The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes.

# CHOOSE: OPTICS (Ordering Points to Identify the Clustering Structure)



<!-- Library -->
demoji                        1.1.0
gensim                        4.2.0
json5                         0.9.5
jsonschema                    4.7.2
matplotlib                    3.5.2
nltk                          3.7
numpy                         1.22.4
pip                           22.2
regex                         2022.7.25
requests                      2.28.1
requests-oauthlib             1.3.1
rsa                           4.9
scikit-image                  0.19.3
scikit-learn                  1.1.1
scipy                         1.9.0
Send2Trash                    1.8.0
setuptools                    63.2.0
six                           1.16.0
sklearn                       0.0
smart-open                    6.0.0
soupsieve                     2.3.2.post1
stack-data                    0.3.0
tabulate                      0.8.10
tensorboard                   2.9.1
tensorboard-data-server       0.6.1
tensorboard-plugin-wit        1.8.1
tensorflow-estimator          2.9.0
tensorflow-macos              2.9.2
tensorflow-metal              0.5.0
termcolor                     1.1.0
terminado                     0.15.0
testpath                      0.6.0
threadpoolctl                 3.1.0
tifffile                      2022.7.28
tornado                       6.2
tqdm                          4.64.0
traitlets                     5.3.0
typer                         0.3.2
typing_extensions             4.3.0
urllib3                       1.26.11
wcwidth                       0.2.5
webencodings                  0.5.1
Werkzeug                      2.2.0
wheel                         0.37.1
widgetsnbextension            3.6.1
wrapt                         1.14.1
zipp                          3.8.1